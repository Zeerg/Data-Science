{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly taken from Malware Science Book and updated some things\n",
    "\n",
    "https://www.malwaredatascience.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "def my_model_simple(input_length=1024):\n",
    "    input = layers.Input(shape=(input_length,), dtype='float32')\n",
    "    middle = layers.Dense(units=512, activation='relu')(input)\n",
    "    output = layers.Dense(units=1, activation='sigmoid')(middle)\n",
    "\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def my_model(input_length=1024):\n",
    "    # Note that we can name any layer by passing it a \"name\" argument.\n",
    "    input = layers.Input(shape=(input_length,), dtype='float32', name='input')\n",
    "\n",
    "    # We stack a deep densely-connected network on tops\n",
    "    x = layers.Dense(2048, activation='relu')(input)\n",
    "    x = layers.normalization.BatchNormalization()(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.normalization.BatchNormalization()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.normalization.BatchNormalization()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.normalization.BatchNormalization()(x)\n",
    "\n",
    "    # And finally we add the last (logistic regression) layer:\n",
    "    output = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_plot(fpr, tpr, path_to_file):\n",
    "    \"\"\"\n",
    "    :param fpr: array of false positive rates (an output from metrics.roc_curve())\n",
    "    :param tpr: array of true positive rates (an output from metrics.roc_curve())\n",
    "    :param path_to_file: where you wish to save the .png file\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "    ax.get_xaxis().set_minor_locator(matplotlib.ticker.AutoMinorLocator())\n",
    "    ax.get_yaxis().set_minor_locator(matplotlib.ticker.AutoMinorLocator())\n",
    "    ax.grid(b=True, which='major', color='w', linewidth=1.0)\n",
    "    ax.grid(b=True, which='minor', color='w', linewidth=0.5)\n",
    "\n",
    "    plt.semilogx(fpr, tpr, 'b-', label=\"Test set\")\n",
    "    plt.savefig(path_to_file)\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import callbacks\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class MyCallback(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Custom Keras callback to print validation AUC metric during training.\n",
    "    Allowable over-writable methods:\n",
    "    on_epoch_begin, on_epoch_end, on_batch_begin, on_batch_end,\n",
    "    on_train_begin, on_train_end\n",
    "    \"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        validation_labels = self.validation_data[1]\n",
    "        validation_scores = self.model.predict(self.validation_data[0])\n",
    "        # flatten the scores:\n",
    "        validation_scores = [el[0] for el in validation_scores]\n",
    "        fpr, tpr, thres = metrics.roc_curve(y_true=validation_labels,\n",
    "                                            y_score=validation_scores)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        print('\\n\\tEpoch {}, Validation AUC = {}'.format(epoch,\n",
    "                                                         np.round(auc, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "7/6 [=================================] - 18s 3s/step - loss: 0.5492 - accuracy: 0.7506\n",
      "Epoch 2/10\n",
      "7/6 [=================================] - 40s 6s/step - loss: 0.2099 - accuracy: 0.9126\n",
      "Epoch 3/10\n",
      "7/6 [=================================] - 41s 6s/step - loss: 0.1170 - accuracy: 0.9596\n",
      "Epoch 4/10\n",
      "7/6 [=================================] - 24s 3s/step - loss: 0.1138 - accuracy: 0.9573\n",
      "Epoch 5/10\n",
      "7/6 [=================================] - 22s 3s/step - loss: 0.0864 - accuracy: 0.9718\n",
      "Epoch 6/10\n",
      "7/6 [=================================] - 36s 5s/step - loss: 0.1034 - accuracy: 0.9628\n",
      "Epoch 7/10\n",
      "7/6 [=================================] - 35s 5s/step - loss: 0.0396 - accuracy: 0.9881\n",
      "Epoch 8/10\n",
      "7/6 [=================================] - 20s 3s/step - loss: 0.0578 - accuracy: 0.9812\n",
      "Epoch 9/10\n",
      "7/6 [=================================] - 19s 3s/step - loss: 0.0387 - accuracy: 0.9883\n",
      "Epoch 10/10\n",
      "7/6 [=================================] - 31s 4s/step - loss: 0.0278 - accuracy: 0.9923\n",
      "[0.5928085, 0.9893746, 0.08666548, 0.04018569, 0.015443385, 0.7545063, 0.9999971, 0.24936265, 0.8646091, 0.99387914, 0.9999896, 0.5584367, 0.5826191, 0.32542306, 0.09676108, 0.99999714, 0.10852009, 0.028113931, 0.004361123, 0.334956, 0.08666548, 0.028113931, 0.010056436, 0.0039224327, 0.0039224327, 0.7666678, 0.24895257, 0.9939281, 0.69319624, 0.0039224327, 0.38869655, 0.9999963, 0.889449, 0.9999896, 0.5755563, 0.96055233, 0.7541746, 0.9999859, 0.7978991, 0.23959291, 0.110082775, 0.99387914, 0.795136, 0.04727465, 0.015738934, 0.110082775, 0.014317721, 0.028113931, 0.015443385, 0.24936265, 0.7515396, 0.7978991, 0.23946488, 0.7349657, 0.99527884, 0.23959291, 0.7545063, 0.71172106, 0.032189667, 0.0070393085, 0.004361123, 0.8646091, 0.7666678, 0.9746903, 0.5826191, 0.9999963, 0.23946488, 0.9939281, 0.9999971, 0.7349657, 0.010048389, 0.7978991, 0.013908386, 0.13377944, 0.5755563, 0.334956, 0.9939281, 0.889449, 0.87684685, 0.041002154, 0.010056436, 0.008945733, 0.008945733, 0.69319624, 0.3215278, 0.0039224327, 0.50521547, 0.795136, 0.008742899, 0.99999106, 0.99999714, 0.010752499, 0.013754249, 0.9999963, 0.7666678, 0.0039224327, 0.23946488, 0.72279775, 0.046046615, 0.889449]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import mmh3\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def read_file(sha, dir):\n",
    "    with open(os.path.join(dir, sha), 'r', encoding='latin1') as fp:\n",
    "        file = fp.read()\n",
    "    return file\n",
    "\n",
    "\n",
    "def extract_features(sha, path_to_files_dir,\n",
    "                     hash_dim=1024, split_regex=r\"\\s+\"):\n",
    "    # first, read in the file as a big string:\n",
    "    file = read_file(sha=sha, dir=path_to_files_dir)\n",
    "    # next, split the big string into a bunch of different tokens (\"words\"):\n",
    "    tokens = re.split(pattern=split_regex, string=file)\n",
    "    # now take the module(hash of each token) so that each token is replaced\n",
    "    # by bucket (category) from 1:hash_dim.\n",
    "    token_hash_buckets = [\n",
    "        (mmh3.hash(w) % (hash_dim - 1) + 1) for w in tokens\n",
    "    ]\n",
    "    # Finally, we'll count how many hits each bucket got, so that our features\n",
    "    # always have length hash_dim, regardless of the size of the HTML file:\n",
    "    token_bucket_counts = np.zeros(hash_dim)\n",
    "    # this returns the frequency counts for each unique value in\n",
    "    # token_hash_buckets:\n",
    "    buckets, counts = np.unique(token_hash_buckets, return_counts=True)\n",
    "    # and now we insert these counts into our token_bucket_counts object:\n",
    "    for bucket, count in zip(buckets, counts):\n",
    "        token_bucket_counts[bucket] = count\n",
    "    return np.array(token_bucket_counts)\n",
    "\n",
    "\n",
    "def my_generator(benign_files, malicious_files,\n",
    "                 path_to_benign_files, path_to_malicious_files,\n",
    "                 batch_size, features_length=1024):\n",
    "    n_samples_per_class = batch_size / 2\n",
    "    assert len(benign_files) >= n_samples_per_class\n",
    "    assert len(malicious_files) >= n_samples_per_class\n",
    "    while True:\n",
    "        # first, extract features for some random benign files:\n",
    "        ben_features = [\n",
    "            extract_features(sha, path_to_files_dir=path_to_benign_files,\n",
    "                             hash_dim=features_length)\n",
    "            for sha in np.random.choice(benign_files, int(n_samples_per_class),\n",
    "                                        replace=False)\n",
    "        ]\n",
    "        # now do the same for some malicious files:\n",
    "        mal_features = [\n",
    "            extract_features(sha, path_to_files_dir=path_to_malicious_files,\n",
    "                             hash_dim=features_length)\n",
    "            for sha in np.random.choice(malicious_files, int(n_samples_per_class),\n",
    "                                        replace=False)\n",
    "        ]\n",
    "        # concatenate these together to get our features and labels array:\n",
    "        all_features = ben_features + mal_features\n",
    "        # \"0\" will represent \"benign\", and \"1\" will represent \"malware\":\n",
    "        labels = [0 for i in range(int(n_samples_per_class))] + [1 for i in range(int(\n",
    "            n_samples_per_class))]\n",
    "\n",
    "        # finally, let's shuffle the labels and features so that the ordering\n",
    "        # is not always benign, then malware:\n",
    "        idx = np.random.choice(range(batch_size), batch_size)\n",
    "        all_features = np.array([np.array(all_features[i]) for i in idx])\n",
    "        labels = np.array([labels[i] for i in idx])\n",
    "        yield all_features, labels\n",
    "\n",
    "\n",
    "def make_training_data_generator(features_length, batch_size):\n",
    "    path_to_training_benign_files = 'malware_data_science/ch11/chapter_11_UNDER_40/data/html/benign_files/training/'\n",
    "    path_to_training_malicious_files = 'malware_data_science/ch11/chapter_11_UNDER_40/data/html/malicious_files/training/'\n",
    "\n",
    "    train_benign_files = os.listdir(path_to_training_benign_files)\n",
    "    train_malicious_files = os.listdir(path_to_training_malicious_files)\n",
    "\n",
    "    training_generator = my_generator(\n",
    "        benign_files=train_benign_files,\n",
    "        malicious_files=train_malicious_files,\n",
    "        path_to_benign_files=path_to_training_benign_files,\n",
    "        path_to_malicious_files=path_to_training_malicious_files,\n",
    "        batch_size=batch_size,\n",
    "        features_length=features_length\n",
    "    )\n",
    "    return training_generator\n",
    "\n",
    "\n",
    "def get_validation_data(features_length, n_validation_files):\n",
    "    path_to_validation_benign_files = 'malware_data_science/ch11/chapter_11_UNDER_40/data/html/benign_files/validation/'\n",
    "    path_to_validation_malicious_files = 'malware_data_science/ch11/chapter_11_UNDER_40/data/html/malicious_files/validation/'\n",
    "    # get the validation keys:\n",
    "    val_benign_files = os.listdir(path_to_validation_benign_files)\n",
    "    val_malicious_files = os.listdir(path_to_validation_malicious_files)\n",
    "\n",
    "    # create the model:\n",
    "    # grab the validation data and extract the features:\n",
    "    validation_data = my_generator(\n",
    "        benign_files=val_benign_files,\n",
    "        malicious_files=val_malicious_files,\n",
    "        path_to_benign_files=path_to_validation_benign_files,\n",
    "        path_to_malicious_files=path_to_validation_malicious_files,\n",
    "        batch_size=n_validation_files,\n",
    "        features_length=features_length\n",
    "    ).__next__()\n",
    "    return validation_data\n",
    "\n",
    "\n",
    "def example_code_with_validation_data(model, training_generator, steps_per_epoch, features_length, n_validation_files):\n",
    "    validation_data = get_validation_data(features_length, n_validation_files)\n",
    "    model.fit_generator(\n",
    "        validation_data=validation_data,\n",
    "        generator=training_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=10,\n",
    "        verbose=1)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "features_length = 1024\n",
    "# by convention, num_obs_per_epoch should be roughly equal to the size\n",
    "# of your training dataset, but we're making it small here since this\n",
    "# is example code and we want it to run fast!\n",
    "num_obs_per_epoch = 500000\n",
    "batch_size = 8000\n",
    "\n",
    "# create the model using the function from the model architecture section:\n",
    "model = my_model(input_length=features_length)\n",
    "\n",
    "# make the training data generator:\n",
    "training_generator = make_training_data_generator(batch_size=batch_size, features_length=features_length)\n",
    "# and now train the model:\n",
    "model.fit(training_generator, steps_per_epoch=num_obs_per_epoch / batch_size, epochs=10, workers=4, use_multiprocessing=True)\n",
    "\n",
    "# Get validation or unseen dat\n",
    "validation_data = get_validation_data(n_validation_files=100, features_length=1024)\n",
    "validation_labels = validation_data[1]\n",
    "validation_scores = [el[0] for el in model.predict(validation_data[0])]\n",
    "# Evaluate the model with unseen data\n",
    "print(validation_scores)\n",
    "\n",
    "# save the model\n",
    "model.save('my_model.h5')\n",
    "# load the model back into memory from the file:\n",
    "#same_model = load_model('my_model.h5')  # from keras.models.load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC = 0.8771508603441377\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thres = metrics.roc_curve(y_true=validation_labels, y_score=validation_scores)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print('Validation AUC = {}'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "* https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab\n",
    "* https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a\n",
    "* https://keras.io/api/optimizers/sgd/\n",
    "* https://arxiv.org/abs/1412.6980 - Adam Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
